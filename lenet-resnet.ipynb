{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30588,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install torchmetrics\n",
    "!git clone https://github.com/m0hssn/Metrica.git\n",
    "from Metrica.metrica import Metrica"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-01T17:31:29.089408Z",
     "iopub.execute_input": "2023-12-01T17:31:29.090085Z",
     "iopub.status.idle": "2023-12-01T17:31:49.490164Z",
     "shell.execute_reply.started": "2023-12-01T17:31:29.090039Z",
     "shell.execute_reply": "2023-12-01T17:31:49.489230Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.2.0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.24.3)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.9.0)\nRequirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (21.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.1->lightning-utilities>=0.8.0->torchmetrics) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\nCloning into 'Metrica'...\nremote: Enumerating objects: 16, done.\u001B[K\nremote: Counting objects: 100% (16/16), done.\u001B[K\nremote: Compressing objects: 100% (15/15), done.\u001B[K\nremote: Total 16 (delta 5), reused 0 (delta 0), pack-reused 0\u001B[K\nReceiving objects: 100% (16/16), 5.39 KiB | 1.80 MiB/s, done.\nResolving deltas: 100% (5/5), done.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overview of GoogLeNet Model – CNN Architecture\n",
    "\n",
    "The GoogLeNet, also known as Inception V1, emerged from collaborative research by Google and various universities in 2014. Its unveiling in the paper titled “Going Deeper with Convolutions” marked its victory at the ILSVRC 2014 image classification challenge. Notably surpassing prior winners like AlexNet (ILSVRC 2012) and ZF-Net (ILSVRC 2013), as well as yielding a considerably lower error rate than VGG (the 2014 runner-up), this architecture introduced innovations such as 1×1 convolutions positioned within the architecture and global average pooling.\n",
    "\n",
    "#### Key Aspects of GoogLeNet:\n",
    "\n",
    "The GoogLeNet architecture markedly diverges from preceding state-of-the-art models like AlexNet and ZF-Net. Its distinctive features, including 1×1 convolution and global average pooling, facilitate the creation of a deeper architecture. Here are the pivotal elements within the architecture:\n",
    "\n",
    "- 1×1 Convolution: The Inception architecture incorporates 1×1 convolutions to reduce the number of parameters, thereby deepening the architecture. For instance, by employing 1×1 convolutions in intermediate stages, it significantly cuts down the total number of operations compared to traditional convolutions without compromising performance.\n",
    "  \n",
    "  ![Example Image](https://media.geeksforgeeks.org/wp-content/uploads/20200429201100/without1x1.png){width=50%}\n",
    "\n",
    "  - Total Number of operations without 1×1 convolutions: 112.9 M\n",
    "  - With 1×1 convolution: 5.3M, a significant reduction.\n",
    "\n",
    "- Global Average Pooling: In contrast to earlier architectures like AlexNet, GoogLeNet employs global average pooling at the network's end. This strategic layer condenses a 7×7 feature map to 1×1, effectively decreasing trainable parameters to zero and enhancing top-1 accuracy by 0.6%.\n",
    "\n",
    "- Inception Module: Unique to this architecture, the Inception module features parallel 1×1, 3×3, 5×5 convolutions and 3×3 max pooling. These operations, stacked together, allow the handling of objects at multiple scales more efficiently.\n",
    "\n",
    "  ![Conv Layer](https://media.geeksforgeeks.org/wp-content/uploads/20200429201304/Incepption-module.PNG)\n",
    "\n",
    "- Auxiliary Classifier for Training: GoogLeNet integrates intermediate classifier branches within the architecture solely for training purposes. These branches comprise various layers aimed at combating gradient vanishing and providing regularization, thereby contributing to the overall loss with a weight of 0.3.\n",
    "\n",
    "- Model Architecture: Spanning 22 layers, the design prioritizes computational efficiency, making it feasible for implementation even on devices with limited computational resources. It includes two auxiliary classifier layers connected to the output of Inception (4a) and Inception (4d) layers.\n",
    "\n",
    "  ![Google-Net](https://media.geeksforgeeks.org/wp-content/uploads/20200429201549/Inceptionv1_architecture.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Auxiliary(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Auxiliary, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        self.one_by_one = ConvBlock(im_channels, num_1x1, kernel_size=1)\n",
    "        \n",
    "        self.tree_by_three_red = ConvBlock(im_channels, num_3x3_red, kernel_size=1)  \n",
    "        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.five_by_five_red = ConvBlock(im_channels, num_5x5_red, kernel_size=1)\n",
    "        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_proj = ConvBlock(im_channels, num_pool_proj, kernel_size=1)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x1 = self.one_by_one(x)\n",
    "        \n",
    "        x2 = self.tree_by_three_red(x)\n",
    "        x2 = self.tree_by_three(x2)\n",
    "        \n",
    "        x3 = self.five_by_five_red(x)\n",
    "        x3 = self.five_by_five(x3)\n",
    "        \n",
    "        x4 = self.maxpool(x)\n",
    "        x4 = self.pool_proj(x4)\n",
    "        \n",
    "        x = torch.cat([x1, x2, x3, x4], 1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, use_auxiliary=True, num_classes=1000):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.auxiliary4a = Auxiliary(512, num_classes)\n",
    "        \n",
    "        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        self.use_auxiliary = use_auxiliary\n",
    "        if use_auxiliary:\n",
    "            self.auxiliary4d = Auxiliary(528, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        auxiliary_outputs = []\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        auxiliary_outputs.append(self.auxiliary4a(x))\n",
    "        \n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        if self.use_auxiliary:\n",
    "            auxiliary_outputs.append(self.auxiliary4d(x))\n",
    "        \n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        if self.use_auxiliary:\n",
    "            return x, auxiliary_outputs\n",
    "        else:\n",
    "            return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T16:27:40.913417Z",
     "iopub.execute_input": "2023-12-01T16:27:40.913988Z",
     "iopub.status.idle": "2023-12-01T16:27:40.943565Z",
     "shell.execute_reply.started": "2023-12-01T16:27:40.913952Z",
     "shell.execute_reply": "2023-12-01T16:27:40.942658Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T16:27:40.945017Z",
     "iopub.execute_input": "2023-12-01T16:27:40.945397Z",
     "iopub.status.idle": "2023-12-01T16:27:50.482290Z",
     "shell.execute_reply.started": "2023-12-01T16:27:40.945365Z",
     "shell.execute_reply": "2023-12-01T16:27:50.481306Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 170498071/170498071 [00:05<00:00, 29709000.29it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = LeNet(in_channels=3, num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)  "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T16:53:13.150019Z",
     "iopub.execute_input": "2023-12-01T16:53:13.150860Z",
     "iopub.status.idle": "2023-12-01T16:53:13.246139Z",
     "shell.execute_reply.started": "2023-12-01T16:53:13.150825Z",
     "shell.execute_reply": "2023-12-01T16:53:13.245411Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs=5, use_auxiliary=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (training)\", leave=False)\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        \n",
    "#         test_metrica = Metrica(num_classes=10)\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if use_auxiliary:\n",
    "                outputs, aux_outs = model(inputs)\n",
    "                loss = criterion(outputs, labels) + 0.3 * criterion(aux_outs[0], labels) + 0.3 * criterion(aux_outs[1], labels)\n",
    "            else:\n",
    "                outputs, _, _ = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({\"loss\": running_loss / (i+1),\n",
    "                                    \"accuracy\": 100 * running_correct / running_total})\n",
    "        model.eval()\n",
    "        running_loss_test = 0.0\n",
    "        running_total_test = 0\n",
    "        running_correct_test = 0\n",
    "        \n",
    "        test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (testing)\", leave=False)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(test_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            if use_auxiliary:\n",
    "                outputs, aux_outs = model(inputs)\n",
    "                loss = criterion(outputs, labels) + 0.3 * criterion(aux_outs[0], labels) + 0.3 * criterion(aux_outs[1], labels)\n",
    "            else:\n",
    "                outputs, _, _ = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "#             test_metrica.upgrade(outputs, labels)\n",
    "\n",
    "            running_loss_test += loss.item()\n",
    "            running_total_test += labels.size(0)\n",
    "            running_correct_test += (preds == labels).sum().item()\n",
    "            \n",
    "            test_pbar.set_postfix({\"loss\": running_loss_test / (i+1),\n",
    "                                    \"accuracy\": 100 * running_correct_test / running_total_test})\n",
    "        print(f'train acc = {100 * running_correct / running_total}, test acc: {100 * running_correct_test / running_total_test}')\n",
    "#         test_metrica.print_metrics()\n",
    "# metrica is a method to show classification scores but theres a problem there i will fix later since we save all the predicted values and the actual ones it takes alot of ram so something must be done about that"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T16:53:14.110601Z",
     "iopub.execute_input": "2023-12-01T16:53:14.111206Z",
     "iopub.status.idle": "2023-12-01T16:53:14.125610Z",
     "shell.execute_reply.started": "2023-12-01T16:53:14.111165Z",
     "shell.execute_reply": "2023-12-01T16:53:14.124788Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_model(model, criterion, optimizer, trainloader, testloader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T16:53:14.678793Z",
     "iopub.execute_input": "2023-12-01T16:53:14.679107Z",
     "iopub.status.idle": "2023-12-01T17:11:43.908883Z",
     "shell.execute_reply.started": "2023-12-01T16:53:14.679083Z",
     "shell.execute_reply": "2023-12-01T17:11:43.907874Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                                 \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 46.646, test acc: 58.25\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                 \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 61.438, test acc: 59.1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                 \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 68.202, test acc: 68.38\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                 \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 72.864, test acc: 70.73\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                 \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 76.334, test acc: 71.4\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ResNet Architecture](https://www.researchgate.net/publication/354224999/figure/fig1/AS:1126083944562688@1645490742169/The-structure-of-the-ResNet34-CNN-Network-The-input-of-the-network-is-the-preprocessed.png)\n",
    "\n",
    "### Solving the Vanishing Gradients Issue with ResNet:\n",
    "\n",
    "![Residual connection](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2F%40achronus%2Fexploring-residual-connections-in-transformers-2cd18b9e35eb&psig=AOvVaw2L2R1QdOpqJ-Jz9ybSbyhZ&ust=1701533098466000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCMDp94HP7oIDFQAAAAAdAAAAABAI)\n",
    "\n",
    "ResNet50's architecture introduced skip connections, also termed residual connections, as a fundamental aspect. These connections were pivotal in enabling the network to delve into deeper architectures while mitigating the problem of vanishing gradients.\n",
    "\n",
    "The issue of vanishing gradients arises during the training of deep neural networks when the gradients in the deeper layers diminish significantly, hindering the learning process for those layers. This challenge exacerbates as the network's depth increases.\n",
    "\n",
    "Skip connections circumvent this challenge by allowing information to flow directly from the input to the output of the network, circumventing one or more intermediate layers. This facilitates the learning of residual functions that map the input to the desired output, alleviating the need to learn the entire mapping from scratch.\n",
    "\n",
    "\n",
    "\n",
    "In the image above, alongside the standard connections, a direct connection bypassing certain layers in the model is depicted (skip connection). With this, the output transforms from h(x) = f(wx +b) to h(x) = f(x) + x. These skip connections act as alternate pathways for gradients to flow, providing valuable shortcuts for learning.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:31:49.491905Z",
     "iopub.execute_input": "2023-12-01T17:31:49.492385Z",
     "iopub.status.idle": "2023-12-01T17:31:49.502746Z",
     "shell.execute_reply.started": "2023-12-01T17:31:49.492354Z",
     "shell.execute_reply": "2023-12-01T17:31:49.501752Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet34, self).__init__()\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        self.conv1 = ConvBlock(3, 64, kernel_size = 7, stride = 2, padding = 3)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),)\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:31:49.503943Z",
     "iopub.execute_input": "2023-12-01T17:31:49.504273Z",
     "iopub.status.idle": "2023-12-01T17:31:49.517280Z",
     "shell.execute_reply.started": "2023-12-01T17:31:49.504241Z",
     "shell.execute_reply": "2023-12-01T17:31:49.516528Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    mean=[0.4914, 0.4822, 0.4465],\n",
    "    std=[0.2023, 0.1994, 0.2010],),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False) # set to 32 because of memory reasons\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:31:49.519610Z",
     "iopub.execute_input": "2023-12-01T17:31:49.520043Z",
     "iopub.status.idle": "2023-12-01T17:32:11.031911Z",
     "shell.execute_reply.started": "2023-12-01T17:31:49.520007Z",
     "shell.execute_reply": "2023-12-01T17:32:11.030879Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 170498071/170498071 [00:16<00:00, 10055922.46it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = ResNet34(ResidualBlock, [3, 4, 6, 3])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)  "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:32:11.033266Z",
     "iopub.execute_input": "2023-12-01T17:32:11.033915Z",
     "iopub.status.idle": "2023-12-01T17:32:11.209639Z",
     "shell.execute_reply.started": "2023-12-01T17:32:11.033880Z",
     "shell.execute_reply": "2023-12-01T17:32:11.208860Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_resnet_model(model, criterion, optimizer, train_loader, test_loader, num_epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (training)\", leave=False)\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        \n",
    "#         test_metrica = Metrica(num_classes=10)\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({\"loss\": running_loss / (i+1),\n",
    "                                    \"accuracy\": 100 * running_correct / running_total})\n",
    "        model.eval()\n",
    "        running_loss_test = 0.0\n",
    "        running_total_test = 0\n",
    "        running_correct_test = 0\n",
    "        \n",
    "        test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (testing)\", leave=False)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(test_pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             test_metrica.upgrade(outputs, labels)\n",
    "\n",
    "            running_loss_test += loss.item()\n",
    "            running_total_test += labels.size(0)\n",
    "            running_correct_test += (preds == labels).sum().item()\n",
    "            \n",
    "            test_pbar.set_postfix({\"loss\": running_loss_test / (i+1),\n",
    "                                    \"accuracy\": 100 * running_correct_test / running_total_test})\n",
    "        print(f'train acc = {100 * running_correct / running_total}, test acc: {100 * running_correct_test / running_total_test}')\n",
    "\n",
    "#         test_metrica.print_metrics()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:32:11.210848Z",
     "iopub.execute_input": "2023-12-01T17:32:11.211127Z",
     "iopub.status.idle": "2023-12-01T17:32:11.223095Z",
     "shell.execute_reply.started": "2023-12-01T17:32:11.211103Z",
     "shell.execute_reply": "2023-12-01T17:32:11.222239Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_resnet_model(model, criterion, optimizer, trainloader, testloader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T17:32:11.224282Z",
     "iopub.execute_input": "2023-12-01T17:32:11.224548Z",
     "iopub.status.idle": "2023-12-01T17:52:09.947147Z",
     "shell.execute_reply.started": "2023-12-01T17:32:11.224524Z",
     "shell.execute_reply": "2023-12-01T17:52:09.946251Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                                   \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 50.45, test acc: 63.21\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                    \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 70.654, test acc: 72.81\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                    \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 78.396, test acc: 79.06\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                    \r",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 82.612, test acc: 80.65\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "                                                                                                    ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "train acc = 85.654, test acc: 82.24\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\r",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
